{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Implementation\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "In this project, you'll build a **RAG (Retrieval-Augmented Generation)** system that can answer questions about your documents.\n",
    "\n",
    "### What You'll Learn:\n",
    "- âœ… File I/O (reading documents)\n",
    "- âœ… String manipulation (text chunking)\n",
    "- âœ… Functions and parameters\n",
    "- âœ… Lists and dictionaries\n",
    "- âœ… Loops and conditionals\n",
    "- âœ… Basic calculations and statistics\n",
    "\n",
    "### What's Provided for You:\n",
    "- âœ… Embedding model (converts text to numbers)\n",
    "- âœ… Vector database (stores and searches embeddings)\n",
    "- âœ… LLM connection (generates answers)\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-built helper module\n",
    "from rag_helpers import (\n",
    "    EmbeddingModel,\n",
    "    VectorDatabase,\n",
    "    LLM,\n",
    "    Timer,\n",
    "    print_separator,\n",
    "    print_search_results,\n",
    "    print_rag_answer,\n",
    "    check_setup\n",
    ")\n",
    "\n",
    "# Import standard Python libraries you'll use\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# Check if everything is installed correctly\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "Set up the basic settings for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_FOLDER = \"./team_docs_test\"\n",
    "\n",
    "CHUNK_SIZE = 500     \n",
    "OVERLAP = 50          \n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Documents folder: {DOCS_FOLDER}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"  Overlap: {OVERLAP} characters\")\n",
    "print(f\"  Top-K results: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Loading\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through all `.txt` files in the folder\n",
    "2. Read each file's content\n",
    "3. Store the content and filename in a dictionary\n",
    "4. Return a list of these dictionaries\n",
    "\n",
    "**Python concepts:** File I/O, loops, dictionaries, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(folder_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load all text documents from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to folder containing .txt files\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'content': the text content of the file\n",
    "        - 'filename': the name of the file\n",
    "\n",
    "    Example:\n",
    "        [\n",
    "            {'content': 'This is doc 1...', 'filename': 'doc1.txt'},\n",
    "            {'content': 'This is doc 2...', 'filename': 'doc2.txt'}\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    documents = [] \n",
    "\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        doc_dict = {\n",
    "            'content': content,\n",
    "            'filename': file_path.name\n",
    "        }\n",
    "        documents.append(doc_dict)\n",
    "\n",
    "    print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = load_documents(DOCS_FOLDER)\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\nFirst document: {documents[0]['filename']}\")\n",
    "    print(f\"Content preview: {documents[0]['content'][:200]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  No documents loaded! Check your folder path.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Chunking Function\n",
    "\n",
    "**Why?** Long documents are too big for embeddings. We need to split them into smaller pieces.\n",
    "\n",
    "**What to do:**\n",
    "1. Start at the beginning of the text\n",
    "2. Take a chunk of `chunk_size` characters\n",
    "3. Move forward by `chunk_size - overlap` characters\n",
    "4. Repeat until you reach the end\n",
    "\n",
    "**Python concepts:** String slicing, loops, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \n",
    "    chunks = []  \n",
    "    position = 0 \n",
    "\n",
    "    step = chunk_size - overlap\n",
    "    if step <= 0:\n",
    "        raise ValueError('chunk_size must be greater than overlap.')\n",
    "        \n",
    "    while position < len(text):\n",
    "        chunk = text[position : position + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        position += step\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "test_text = \"This is a test. \" * 50  # Create a long test string\n",
    "test_chunks = chunk_text(test_text, chunk_size=100, overlap=20)\n",
    "\n",
    "print(f\"Test text length: {len(test_text)} characters\")\n",
    "print(f\"Number of chunks: {len(test_chunks)}\")\n",
    "print(f\"\\nFirst chunk: {test_chunks[0]}\")\n",
    "if len(test_chunks) > 1:\n",
    "    print(f\"Second chunk: {test_chunks[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Process All Documents into Chunks\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through each document\n",
    "2. Chunk the document's content\n",
    "3. For each chunk, create metadata (which file it came from, which chunk number)\n",
    "4. Store everything in a list\n",
    "\n",
    "**Python concepts:** Nested loops, dictionaries, enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents: List[Dict[str, str]],\n",
    "                     chunk_size: int = 500,\n",
    "                     overlap: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Process all documents into chunks with metadata.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dictionaries\n",
    "        chunk_size: Size of each chunk\n",
    "        overlap: Overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (chunk_texts, chunk_metadatas)\n",
    "        - chunk_texts: List of chunk strings\n",
    "        - chunk_metadatas: List of metadata dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    chunk_texts = []\n",
    "    chunk_metadatas = []\n",
    "\n",
    "    for doc in documents:\n",
    "        content = doc['content']\n",
    "        filename = doc['filename']\n",
    "        chunks = chunk_text(content, chunk_size, overlap)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_texts.append(chunk)\n",
    "            metadata = {\n",
    "                'source': filename,\n",
    "                'chunk_id': idx\n",
    "            }\n",
    "            chunk_metadatas.append(metadata)\n",
    "\n",
    "    print(f\"âœ“ Created {len(chunk_texts)} chunks from {len(documents)} documents\")\n",
    "    return chunk_texts, chunk_metadatas\n",
    "\n",
    "chunk_texts, chunk_metadatas = process_documents(documents, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "if chunk_texts:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"  Source: {chunk_metadatas[0]['source']}\")\n",
    "    print(f\"  Chunk ID: {chunk_metadatas[0]['chunk_id']}\")\n",
    "    print(f\"  Text: {chunk_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Built: Create Embeddings and Store in Database\n",
    "\n",
    "This part uses the pre-built helpers. Just run these cells - no coding needed! âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model (pre-built)\n",
    "print(\"Initializing embedding model...\")\n",
    "embedder = EmbeddingModel()\n",
    "\n",
    "# Create embeddings for all chunks (pre-built)\n",
    "print(\"\\nCreating embeddings...\")\n",
    "embeddings = embedder.embed_multiple(chunk_texts)\n",
    "print(f\"âœ“ Created {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector database (pre-built)\n",
    "print(\"Initializing vector database...\")\n",
    "vector_db = VectorDatabase()\n",
    "\n",
    "# Add chunks to database (pre-built)\n",
    "print(\"\\nAdding chunks to database...\")\n",
    "vector_db.add_chunks(chunk_texts, embeddings, chunk_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM connection (pre-built)\n",
    "print(\"Connecting to Ollama LLM...\")\n",
    "llm = LLM(model='gemma3:1b-it-qat')\n",
    "\n",
    "# Test the connection\n",
    "print(\"\\nTesting LLM connection...\")\n",
    "if llm.test_connection():\n",
    "    print(\"âœ“ LLM is working!\")\n",
    "else:\n",
    "    print(\"âš ï¸  LLM connection failed! Make sure Docker container is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## RAG Query Function\n",
    "\n",
    "**What to do:**\n",
    "1. Embed the user's question\n",
    "2. Search the database for similar chunks\n",
    "3. Build a prompt with the retrieved context\n",
    "4. Ask the LLM to answer based on the context\n",
    "5. Return the answer and metadata\n",
    "\n",
    "**Python concepts:** Functions, string formatting, dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "    Args:\n",
    "        question: The user's question\n",
    "        top_k: How many chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'question': the original question\n",
    "        - 'answer': the LLM's answer\n",
    "        - 'sources': list of source filenames\n",
    "        - 'contexts': list of retrieved chunks\n",
    "        - 'time': how long it took\n",
    "    \"\"\"\n",
    "\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    # Step 1: Embed question\n",
    "    query_embedding = embedder.embed_text(question)  \n",
    "\n",
    "    # Step 2: Search database\n",
    "    results = vector_db.search(query_embedding, top_k=top_k)  \n",
    "\n",
    "    # Step 3: Extract results\n",
    "    retrieved_chunks = results['documents'][0] \n",
    "    retrieved_metadata = results['metadatas'][0] \n",
    "\n",
    "    # Step 4: Build context\n",
    "    context = \"\\n\\n\".join(retrieved_chunks) \n",
    "\n",
    "    # Step 5: Create prompt (use this template)\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 6: Generate answer\n",
    "    answer = llm.generate_answer(prompt) \n",
    "\n",
    "    # Step 7: Extract sources\n",
    "    sources = [meta.get(\"source\", \"unknown\") for meta in retrieved_metadata]  \n",
    "\n",
    "    # Stop timer\n",
    "    elapsed_time = timer.stop()\n",
    "\n",
    "    # Step 8: Return results\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'contexts': retrieved_chunks,\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ“ RAG query function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your RAG System!\n",
    "\n",
    "Let's try asking some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 1\n",
    "result = rag_query(\"When did the Big Bang occur, according to current measurements?\")\n",
    "# expected_answer: Approximatedly 13.8 billion years\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 2\n",
    "result = rag_query(\"What does the cosmological principle state about the universe?\")\n",
    "# expected_answer: The cosmological principle states that on sufficiently large scales, the universe appears homogeneous and isotropic\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 3\n",
    "result = rag_query(\"Which observation reveals that galaxies recede from Earth at speeds proportional to their distance?\")\n",
    "# expected_answer: The cosmological redshift.\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 4\n",
    "result = rag_query(\"How does the cosmological principle support the use of large-scale surveys when testing models of the universe?\")\n",
    "# expected_answer: The cosmological principle assumes that the universe is homogeneous and isotropic on large scales, \n",
    "# which means that observations from one region can be generalized to others. \n",
    "# This supports the use of large-scale surveys because data gathered from a limited portion of \n",
    "# the sky can still provide valid tests of cosmological models that describe the entire universe.\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 5\n",
    "result = rag_query(\"If the cosmological principle were violated on large scales, what implications would this have for current cosmological models and observations?\")\n",
    "# expected_answer: A violation of the cosmological principle would undermine the foundations of many standard cosmological models,\n",
    "#  which assume large-scale homogeneity and isotropy. It would imply that observations from Earth may not be representative \n",
    "# of the universe as a whole, requiring revised models, reinterpretation of observational data, and potentially \n",
    "# new physics to explain variations in matter distribution or directional differences.\n",
    "\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 6\n",
    "my_question = \"Describe the complete sequence of events from the end of cosmic inflation to the moment of recombination, including how quantum fluctuations grow into large-scale structure, how baryon acoustic oscillations develop, and how matterâ€“radiation decoupling produces the features observed in the cosmic microwave background.\" \n",
    "# expected_answer: After inflation ends, the universe undergoes reheating, during which the vacuum energy that drove inflation decays \n",
    "# .......\n",
    "#  of the sound horizon becomes imprinted as baryon acoustic oscillations in the matter distributionâ€¦\n",
    "\n",
    "result = rag_query(my_question)\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 7\n",
    "result = rag_query(\"What observational evidence first confirmed the existence of cosmic inflation?\")\n",
    "# expected_answer: The first strong observational evidence for cosmic inflation is the BICEP2 experiment's detection of B-mode \n",
    "# polarization in the Cosmic Microwave Background (CMB), a twisting pattern in the light left over from the Big Bang, \n",
    "# which is the signature of primordial gravitational waves generated during the universe's rapid inflation.\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 8\n",
    "result = rag_query(\"What telescope first detected the B-mode polarization pattern that would serve as direct evidence for cosmic inflation?\")\n",
    "# expected_answer: The BICEP2 telescope\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 9\n",
    "result = rag_query(\"Why is it flat?\")\n",
    "# expected_answer: The BICEP2 telescope\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Test Dataset\n",
    "\n",
    "**What to do:**\n",
    "1. Think of 10 questions your documents can answer\n",
    "2. For each question, write the expected answer\n",
    "3. Store them in a structured format\n",
    "\n",
    "**Python concepts:** Lists, dictionaries, data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    # Example (replace with your own!):\n",
    "    {\n",
    "        \"question\": \"Which observation reveals that galaxies recede from Earth at speeds proportional to their distance?\",\n",
    "        \"expected_answer\": \"Hubble's law, which shows a linear relationship between distance and recessional velocity.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What discovery in 1964 provided a snapshot of the universe 380,000 years after the Big Bang?\",\n",
    "        \"expected_answer\": \"The cosmic microwave background radiation, discovered by Penzias and Wilson.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which early-universe process produced helium-4, deuterium, helium-3, and lithium-7 within minutes of the Big Bang?\",\n",
    "        \"expected_answer\": \"Big Bang nucleosynthesis.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What cosmological principle states that the universe is homogeneous and isotropic on large scales?\",\n",
    "        \"expected_answer\": \"The cosmological principle.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which problem in cosmology concerns the extreme uniformity of the cosmic microwave background in regions that should not have been in causal contact?\",\n",
    "        \"expected_answer\": \"The horizon problem.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the term for the early rapid expansion that helps explain the universe's flatness and uniformity?\",\n",
    "        \"expected_answer\": \"Cosmic inflation.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What phenomenon shows that the universe's expansion is accelerating?\",\n",
    "        \"expected_answer\": \"Observations of Type Ia supernovae indicating cosmic acceleration.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which component of the universe accounts for roughly 27 percent of its total mass-energy and interacts only through gravity?\",\n",
    "        \"expected_answer\": \"Dark matter.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"During which epoch did quarks combine to form protons and neutrons?\",\n",
    "        \"expected_answer\": \"Around 10â»â¶ seconds after the Big Bang, during early cooling.\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What event made the universe transparent and allowed photons to travel freely, forming today's cosmic microwave background?\",\n",
    "        \"expected_answer\": \"Recombination, when electrons and nuclei formed neutral atoms.\",\n",
    "        \"category\": \"factual\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"âœ“ Created {len(test_questions)} test questions\")\n",
    "print(f\"\\nExample question:\")\n",
    "print(f\"  Q: {test_questions[0]['question']}\")\n",
    "print(f\"  Expected: {test_questions[0]['expected_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculate Evaluation Metrics\n",
    "\n",
    "**Python concepts:** Functions, calculations, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_latency(results: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average response time.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries (each has 'time' field)\n",
    "\n",
    "    Returns:\n",
    "        Average time in seconds\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    \n",
    "    total_time = 0.0\n",
    "    for r in results:\n",
    "        total_time += r.get(\"time\", 0)\n",
    "\n",
    "    return total_time / len(results)\n",
    "\n",
    "\n",
    "\n",
    "def count_successful_retrievals(results: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count how many queries successfully retrieved context.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Number of successful retrievals\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for r in results:\n",
    "        contexts = r.get(\"contexts\", [])\n",
    "        if contexts:  \n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def get_all_sources(results: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get unique list of all sources used.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of unique source filenames\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    all_sources = set()\n",
    "    for r in results:\n",
    "        sources = r.get(\"sources\", [])\n",
    "        for s in sources:\n",
    "            all_sources.add(s)\n",
    "\n",
    "    return list(all_sources)\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Complete Evaluation\n",
    "\n",
    "**Python concepts:** Loops, function calls, data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(test_questions: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run RAG system on all test questions.\n",
    "\n",
    "    Args:\n",
    "        test_questions: List of test question dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for test in test_questions:\n",
    "        question = test[\"question\"]\n",
    "        result = rag_query(question)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Running evaluation on all test questions...\\n\")\n",
    "all_results = run_evaluation(test_questions)\n",
    "\n",
    "print(f\"\\nâœ“ Completed {len(all_results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Display Results\n",
    "\n",
    "Show the evaluation metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics using your functions\n",
    "avg_latency = calculate_average_latency(all_results)\n",
    "successful_retrievals = count_successful_retrievals(all_results)\n",
    "all_sources_used = get_all_sources(all_results)\n",
    "hit_rate = successful_retrievals / len(all_results) if all_results else 0\n",
    "\n",
    "# Display metrics\n",
    "print_separator(\"EVALUATION RESULTS\")\n",
    "print(f\"\\nTotal Questions Tested: {len(all_results)}\")\n",
    "print(f\"Successful Retrievals: {successful_retrievals}\")\n",
    "print(f\"Hit Rate: {hit_rate:.2%}\")\n",
    "print(f\"Average Latency: {avg_latency:.2f} seconds\")\n",
    "print(f\"\\nSources Used: {', '.join(all_sources_used)}\")\n",
    "print_separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual results\n",
    "print(\"\\nIndividual Test Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"[Test {i}]\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer']}\") # ['answer'][:200]}...\n",
    "    print(f\"Sources: {', '.join(set(result['sources']))}\")\n",
    "    print(f\"Time: {result['time']:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Your Results\n",
    "\n",
    "Save your test results to a JSON file for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "results_summary = {\n",
    "    'metrics': {\n",
    "        'total_questions': len(all_results),\n",
    "        'successful_retrievals': successful_retrievals,\n",
    "        'hit_rate': hit_rate,\n",
    "        'average_latency': avg_latency\n",
    "    },\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Results saved to 'evaluation_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've successfully built a RAG system!\n",
    "\n",
    "### What You Accomplished:\n",
    "âœ… Loaded documents from files  \n",
    "âœ… Chunked text with overlap  \n",
    "âœ… Created a RAG query pipeline  \n",
    "âœ… Built a test dataset  \n",
    "âœ… Calculated evaluation metrics  \n",
    "âœ… Generated a results report  \n",
    "\n",
    "### Next Steps:\n",
    "- Try different chunk sizes and overlaps\n",
    "- Add more test questions\n",
    "- Experiment with different values for `top_k`\n",
    "- Analyze which questions work best\n",
    "- Write up your findings in a report\n",
    "\n",
    "Great job! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
